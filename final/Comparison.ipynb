{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cfaefe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL COMPARISON & ANALYSIS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d53c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "646b9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1: LOAD ALL MODELS & TEST DATA\n",
    "X_test = pd.read_csv('data/X_test.csv')\n",
    "y_test = pd.read_csv('data/y_test.csv').values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21231f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = pickle.load(open('models/logistic_model.pkl', 'rb'))\n",
    "model_rf = pickle.load(open('models/random_forest_model.pkl', 'rb'))\n",
    "model_gb = pickle.load(open('models/gradient_boosting_model.pkl', 'rb'))\n",
    "ensemble = pickle.load(open('models/ensemble_model.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71697d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2: COMPREHENSIVE MODEL COMPARISON\n",
    "models = {\n",
    "    'Logistic Regression': model_lr,\n",
    "    'Random Forest': model_rf,\n",
    "    'Gradient Boosting': model_gb,\n",
    "    'Ensemble (RF+GB)': ensemble\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "97aab211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression... (0.9425)\n",
      "Random Forest... (0.9345)\n",
      "Gradient Boosting... (0.9415)\n",
      "Ensemble (RF+GB)... (0.9410)\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"{name}...\", end=\" \")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba) if y_proba is not None else 0\n",
    "    \n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    }\n",
    "    \n",
    "    print(f\"({accuracy:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9302d7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     Accuracy  Precision  Recall  F1-Score  ROC-AUC\n",
      "Logistic Regression    0.9425     0.9314  0.9636    0.9472   0.9905\n",
      "Random Forest          0.9345     0.9304  0.9486    0.9394   0.9856\n",
      "Gradient Boosting      0.9415     0.9297  0.9636    0.9464   0.9897\n",
      "Ensemble (RF+GB)       0.9410     0.9344  0.9570    0.9456   0.9889\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n\" + results_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6935ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df.to_csv('outputs/model_comparison_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7b095a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\n",
      "  1. Logistic Regression: 0.9425 (94.25%)\n",
      "  2. Gradient Boosting: 0.9415 (94.15%)\n",
      "  3. Ensemble (RF+GB): 0.9410 (94.10%)\n",
      "  4. Random Forest: 0.9345 (93.45%)\n",
      "\n",
      "F1-Score:\n",
      "  1. Logistic Regression: 0.9472\n",
      "  2. Gradient Boosting: 0.9464\n",
      "  3. Ensemble (RF+GB): 0.9456\n",
      "  4. Random Forest: 0.9394\n",
      "\n",
      "BEST MODEL: Logistic Regression\n",
      "Accuracy: 0.9425 (94.25%)\n"
     ]
    }
   ],
   "source": [
    "# PART 3: DETAILED ANALYSIS BY METRIC\n",
    "print(\"\\nAccuracy:\")\n",
    "accuracy_ranking = results_df['Accuracy'].sort_values(ascending=False)\n",
    "for i, (model, acc) in enumerate(accuracy_ranking.items(), 1):\n",
    "    print(f\"  {i}. {model}: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nF1-Score:\")\n",
    "f1_ranking = results_df['F1-Score'].sort_values(ascending=False)\n",
    "for i, (model, f1) in enumerate(f1_ranking.items(), 1):\n",
    "    print(f\"  {i}. {model}: {f1:.4f}\")\n",
    "\n",
    "best_model_name = results_df['Accuracy'].idxmax()\n",
    "best_accuracy = results_df.loc[best_model_name, 'Accuracy']\n",
    "\n",
    "print(f\"\\nBEST MODEL: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c401967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     True Negatives  False Positives  False Negatives  True Positives\n",
      "Logistic Regression             853               76               39            1032\n",
      "Random Forest                   853               76               55            1016\n",
      "Gradient Boosting               851               78               39            1032\n",
      "Ensemble (RF+GB)                857               72               46            1025\n"
     ]
    }
   ],
   "source": [
    "# PART 4: CONFUSION MATRICES\n",
    "cm_data = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    cm_data[name] = {\n",
    "        'True Negatives': cm[0, 0],\n",
    "        'False Positives': cm[0, 1],\n",
    "        'False Negatives': cm[1, 0],\n",
    "        'True Positives': cm[1, 1]\n",
    "    }\n",
    "\n",
    "cm_df = pd.DataFrame(cm_data).T\n",
    "print(\"\\n\" + cm_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aeccc0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Feature Importance:\n",
      "  Actual Wage: 0.8293\n",
      "  Experience: 0.0860\n",
      "  Education: 0.0763\n",
      "  Gender: 0.0084\n",
      "\n",
      "Gradient Boosting Feature Importance:\n",
      "  Actual Wage: 0.9971\n",
      "  Education: 0.0015\n",
      "  Experience: 0.0012\n",
      "  Gender: 0.0002\n",
      "\n",
      "Logistic Regression Coefficients:\n",
      "  Education: -0.0684 Decreases\n",
      "  Gender: -0.0415 Decreases\n",
      "  Experience: 0.0032 Increases\n",
      "  Actual Wage: -0.0014 Decreases\n"
     ]
    }
   ],
   "source": [
    "# PART 5: FEATURE IMPORTANCE (for tree-based models)\n",
    "feature_names = ['Gender', 'Education', 'Experience', 'Actual Wage']\n",
    "\n",
    "print(\"\\nRandom Forest Feature Importance:\")\n",
    "rf_importance = model_rf.feature_importances_\n",
    "for name, imp in sorted(zip(feature_names, rf_importance), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {name}: {imp:.4f}\")\n",
    "\n",
    "print(\"\\nGradient Boosting Feature Importance:\")\n",
    "gb_importance = model_gb.feature_importances_\n",
    "for name, imp in sorted(zip(feature_names, gb_importance), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {name}: {imp:.4f}\")\n",
    "\n",
    "print(\"\\nLogistic Regression Coefficients:\")\n",
    "lr_coef = model_lr.coef_[0]\n",
    "for name, coef in sorted(zip(feature_names, lr_coef), key=lambda x: abs(x[1]), reverse=True):\n",
    "    direction = \"Increases\" if coef > 0 else \"Decreases\"\n",
    "    print(f\"  {name}: {coef:.4f} {direction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d471911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Random Forest': rf_importance,\n",
    "    'Gradient Boosting': gb_importance,\n",
    "    'Avg Importance': (rf_importance + gb_importance) / 2\n",
    "})\n",
    "importance_df = importance_df.sort_values('Avg Importance', ascending=False)\n",
    "importance_df.to_csv('outputs/feature_importance.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e893628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
