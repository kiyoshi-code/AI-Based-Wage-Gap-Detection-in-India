{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cfaefe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL COMPARISON & ANALYSIS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d53c562",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "646b9df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 1: LOAD ALL MODELS & TEST DATA\n",
    "X_test = pd.read_csv('data/X_test.csv')\n",
    "y_test = pd.read_csv('data/y_test.csv').values.ravel()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21231f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_lr = pickle.load(open('models/logistic_model.pkl', 'rb'))\n",
    "model_rf = pickle.load(open('models/random_forest_model.pkl', 'rb'))\n",
    "model_gb = pickle.load(open('models/gradient_boosting_model.pkl', 'rb'))\n",
    "ensemble = pickle.load(open('models/ensemble_model.pkl', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71697d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PART 2: COMPREHENSIVE MODEL COMPARISON\n",
    "models = {\n",
    "    'Logistic Regression': model_lr,\n",
    "    'Random Forest': model_rf,\n",
    "    'Gradient Boosting': model_gb,\n",
    "    'Ensemble (RF+GB)': ensemble\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97aab211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression... (0.9370)\n",
      "Random Forest... (0.9280)\n",
      "Gradient Boosting... (0.9435)\n",
      "Ensemble (RF+GB)... (0.9380)\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"{name}...\", end=\" \")\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_proba = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    roc_auc = roc_auc_score(y_test, y_proba) if y_proba is not None else 0\n",
    "    \n",
    "    results[name] = {\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1-Score': f1,\n",
    "        'ROC-AUC': roc_auc\n",
    "    }\n",
    "    \n",
    "    print(f\"({accuracy:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9302d7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     Accuracy  Precision  Recall  F1-Score  ROC-AUC\n",
      "Logistic Regression    0.9370     0.9408  0.9408    0.9408   0.9891\n",
      "Random Forest          0.9280     0.9331  0.9314    0.9323   0.9852\n",
      "Gradient Boosting      0.9435     0.9577  0.9352    0.9463   0.9896\n",
      "Ensemble (RF+GB)       0.9380     0.9401  0.9436    0.9418   0.9886\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(\"\\n\" + results_df.round(4).to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6935ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_df.to_csv('outputs/model_comparison_results.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b095a89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy:\n",
      "  1. Gradient Boosting: 0.9435 (94.35%)\n",
      "  2. Ensemble (RF+GB): 0.9380 (93.80%)\n",
      "  3. Logistic Regression: 0.9370 (93.70%)\n",
      "  4. Random Forest: 0.9280 (92.80%)\n",
      "\n",
      "F1-Score:\n",
      "  1. Gradient Boosting: 0.9463\n",
      "  2. Ensemble (RF+GB): 0.9418\n",
      "  3. Logistic Regression: 0.9408\n",
      "  4. Random Forest: 0.9323\n",
      "\n",
      "BEST MODEL: Gradient Boosting\n",
      "Accuracy: 0.9435 (94.35%)\n"
     ]
    }
   ],
   "source": [
    "# PART 3: DETAILED ANALYSIS BY METRIC\n",
    "print(\"\\nAccuracy:\")\n",
    "accuracy_ranking = results_df['Accuracy'].sort_values(ascending=False)\n",
    "for i, (model, acc) in enumerate(accuracy_ranking.items(), 1):\n",
    "    print(f\"  {i}. {model}: {acc:.4f} ({acc*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nF1-Score:\")\n",
    "f1_ranking = results_df['F1-Score'].sort_values(ascending=False)\n",
    "for i, (model, f1) in enumerate(f1_ranking.items(), 1):\n",
    "    print(f\"  {i}. {model}: {f1:.4f}\")\n",
    "\n",
    "best_model_name = results_df['Accuracy'].idxmax()\n",
    "best_accuracy = results_df.loc[best_model_name, 'Accuracy']\n",
    "\n",
    "print(f\"\\nBEST MODEL: {best_model_name}\")\n",
    "print(f\"Accuracy: {best_accuracy:.4f} ({best_accuracy*100:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c401967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                     True Negatives  False Positives  False Negatives  True Positives\n",
      "Logistic Regression             873               63               63            1001\n",
      "Random Forest                   865               71               73             991\n",
      "Gradient Boosting               892               44               69             995\n",
      "Ensemble (RF+GB)                872               64               60            1004\n"
     ]
    }
   ],
   "source": [
    "# PART 4: CONFUSION MATRICES\n",
    "cm_data = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    \n",
    "    cm_data[name] = {\n",
    "        'True Negatives': cm[0, 0],\n",
    "        'False Positives': cm[0, 1],\n",
    "        'False Negatives': cm[1, 0],\n",
    "        'True Positives': cm[1, 1]\n",
    "    }\n",
    "\n",
    "cm_df = pd.DataFrame(cm_data).T\n",
    "print(\"\\n\" + cm_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aeccc0e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Feature Importance:\n",
      "  Actual Wage: 0.8348\n",
      "  Experience: 0.0864\n",
      "  Education: 0.0704\n",
      "  Gender: 0.0083\n",
      "\n",
      "Gradient Boosting Feature Importance:\n",
      "  Actual Wage: 0.9965\n",
      "  Experience: 0.0028\n",
      "  Education: 0.0006\n",
      "  Gender: 0.0001\n",
      "\n",
      "Logistic Regression Coefficients:\n",
      "  Education: -0.0682 Decreases\n",
      "  Gender: -0.0406 Decreases\n",
      "  Experience: 0.0075 Increases\n",
      "  Actual Wage: -0.0014 Decreases\n"
     ]
    }
   ],
   "source": [
    "# PART 5: FEATURE IMPORTANCE (for tree-based models)\n",
    "feature_names = ['Gender', 'Education', 'Experience', 'Actual Wage']\n",
    "\n",
    "print(\"\\nRandom Forest Feature Importance:\")\n",
    "rf_importance = model_rf.feature_importances_\n",
    "for name, imp in sorted(zip(feature_names, rf_importance), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {name}: {imp:.4f}\")\n",
    "\n",
    "print(\"\\nGradient Boosting Feature Importance:\")\n",
    "gb_importance = model_gb.feature_importances_\n",
    "for name, imp in sorted(zip(feature_names, gb_importance), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {name}: {imp:.4f}\")\n",
    "\n",
    "print(\"\\nLogistic Regression Coefficients:\")\n",
    "lr_coef = model_lr.coef_[0]\n",
    "for name, coef in sorted(zip(feature_names, lr_coef), key=lambda x: abs(x[1]), reverse=True):\n",
    "    direction = \"Increases\" if coef > 0 else \"Decreases\"\n",
    "    print(f\"  {name}: {coef:.4f} {direction}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d471911a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feature importance\n",
    "importance_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Random Forest': rf_importance,\n",
    "    'Gradient Boosting': gb_importance,\n",
    "    'Avg Importance': (rf_importance + gb_importance) / 2\n",
    "})\n",
    "importance_df = importance_df.sort_values('Avg Importance', ascending=False)\n",
    "importance_df.to_csv('outputs/feature_importance.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e893628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
